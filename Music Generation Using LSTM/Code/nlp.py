import numpy as np
from keras.utils import np_utils
from keras.models import Sequential
from music21 import converter, instrument, note, chord ,stream
import glob
from keras.callbacks import ModelCheckpoint
from keras.layers import Dense, Dropout, LSTM
notes=[]
for file in glob.glob("C:/Users/MY/Desktop/New folder/NLP_Mini Project/Music Generation Using LSTM/Input/alb_esp2.mid"):
	midi=converter.parse(file)
	notes_to_parse=None
	parts=instrument.partitionByInstrument(midi)
	if parts:  #file has instrument parts
		notes_to_parse=parts.parts[0].recurse()
	else:  #file has notes in a flat structure
		notes_to_parse=midi.flat.notes
	for element in notes_to_parse:
		if isinstance(element,note.Note):
			notes.append(str(element.pitch))
		elif isinstance(element,chord.Chord):
			notes.append('.'.join(str(n) for n in element.normalOrder))
unique_chars=sorted(list(set(notes)))

print(unique_chars)
char_to_int={}
int_to_char={}

for i, c in enumerate(unique_chars):
	char_to_int.update({c: i})
	int_to_char.update({i: c})
	#preparing input and output dataset
	X=[]
	Y=[]
for i in range(0, len(notes) - 50, 1):
	sequence=notes[i:i + 50]
	label=notes[i + 50]
	X.append([char_to_int[char] for char in sequence])
	Y.append((char_to_int[label]))
#reshapping, normalizing 
X_modified=np.reshape(X, (len(X), 50, 1))
X_modified=X_modified / float(len(unique_chars))
Y_modified=np_utils.to_categorical(Y)

#defining the LSTM model
model=Sequential()
model.add(LSTM(256, input_shape=(X_modified.shape[1], X_modified.shape[2]), return_sequences=True))
model.add(Dropout(0.2))
model.add(LSTM(256))
model.add(Dropout(0.2))
model.add(Dense(Y_modified.shape[1], activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model.summary()

# fitting the model
model.fit(X_modified, Y_modified, epochs=50, batch_size=50)

# picking a random-seed
# start_index = np.random.randint(0, len(X) - 1)
start_index=1
pattern=X[start_index]
prediction_output=[]

#generating characters
for i in range(150):
	x=np.reshape(pattern, (1, len(pattern), 1))
	x=x / float(len(unique_chars))
	pred_index=np.argmax(model.predict(x, verbose=0))
	char_out=int_to_char[pred_index]
	prediction_output.append(char_out)
	ind=np.asarray([pred_index])
	pattern=np.append(pattern, ind)
	pattern=pattern[1:len(pattern)]
print("output")
print(prediction_output)
offset=0
output_notes=[]

# create note and chord objects based on the values generated by the model

for pattern in prediction_output:
	# pattern is a chord
	if('.' in pattern) or pattern.isdigit():
		notes_in_chord=pattern.split('.')
		notes=[]
		for current_note in notes_in_chord:
			new_note=note.Note(int(current_note))
			new_note.storedInstrument=instrument.Piano()
			notes.append(new_note)
		new_chord=chord.Chord(notes)
		new_chord.offset=offset
		output_notes.append(new_chord)
	# pattern is a note
	else:
		new_note=note.Note(pattern)
		new_note.offset=offset
		new_note.storedInstrument=instrument.Piano()
		output_notes.append(new_note)
	# increase offset each iteration so that notes do not stack offset += 0.5
	offset += 0.5
midi_stream=stream.Stream(output_notes)
midi_stream.write('midi',fp='test_output.mid')